{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnHMtxTUavbx"
      },
      "source": [
        "#@title Author: Michael Evans { display-mode: \"form\" }\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ciecm6Ia2Xa"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook demonstrates a workflow for training a [fully convolutional neural network (FCNN)](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf), specifically [U-net](https://arxiv.org/abs/1505.04597) on previously exctracted remote sensing data using Tensorflow. In this example, we read 256x256 pixel image chips saved as zipped tfrecords in Google Cloud Storage (Note: the data can be read in from anywhere) containing the visible, infrared, and near infrared bands of Sentinel-2 imagery and a binary label band. This relatively simple model is a mostly unmodified version of [this example](https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb) from the TensorFlow docs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yla55CsQa2yw"
      },
      "source": [
        "from os.path import join\n",
        "from sys import path\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDBvGhTXa5II"
      },
      "source": [
        "## Clone repo containing preprocessing and prediction functions\n",
        "!git clone https://github.com/mjevans26/Satellite_ComputerVision.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl2DPfr9a8eW"
      },
      "source": [
        "# Load the necessary modules from repo\n",
        "path.append('/content/Satellite_ComputerVision')\n",
        "\n",
        "from utils.processing import get_training_dataset, get_eval_dataset\n",
        "from utils.model_tools import get_model, weighted_bce, make_confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoYO47K1gllv"
      },
      "source": [
        "# Specify names locations for outputs in Cloud Storage. \n",
        "BUCKET = '{YOUR_GCS BUCKET HERE}'\n",
        "BUCKET_PATH = join('gs://', BUCKET)\n",
        "\n",
        "FOLDER = 'NC_solar'\n",
        "PRED_BASE = 'data/predict'\n",
        "TRAIN_BASE = 'data/training'\n",
        "EVAL_BASE = 'data/eval'\n",
        "\n",
        "# Specify inputs (Sentinel bands) to the model and the response variable.\n",
        "opticalBands = ['B2', 'B3', 'B4']\n",
        "thermalBands = ['B8', 'B11', 'B12']\n",
        "\n",
        "BANDS = opticalBands + thermalBands# + pcaBands\n",
        "RESPONSE = 'landcover'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBQN2UdagYj6"
      },
      "source": [
        "## Training Data\n",
        "First, we will read previously exported training data fro GCS into TFRecordDatasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7biGY7cbBSU"
      },
      "source": [
        "# make sure we have training records\n",
        "trainPattern = join(BUCKET_PATH, FOLDER, TRAIN_BASE, '*.tfrecord.gz')\n",
        "print(trainPattern)\n",
        "trainFiles = !gsutil ls {trainPattern}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e3C9k5Ugm0R"
      },
      "source": [
        "# create training dataset with default arguments for batch (16), repeat (True), and normalization axis (0)\n",
        "training = get_training_dataset(trainFiles, FEATURES_DICT, BANDS, RESPONSE, 2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhylXyE4g2U2"
      },
      "source": [
        "# confirm the training dataset produces expected results\n",
        "iterator = iter(training)\n",
        "print(iterator.next())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnpFtwj_g_lA"
      },
      "source": [
        "evalPattern = join(BUCKET_PATH, FOLDER, EVAL_BASE, '*.tfrecord.gz')\n",
        "print(evalPattern)\n",
        "evalFiles = !gsutil ls {evalPattern}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js6Dn2dshHYL"
      },
      "source": [
        "# create evaluation dataset\n",
        "evaluation = get_eval_dataset(evalFiles, FEATURES_DICT, BANDS, RESPONSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR06Y089jeSk"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ww2Yq36kbJm"
      },
      "source": [
        "# Define Global variables for Model Training\n",
        "EPOCHS = 100\n",
        "\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "METRICS = {\n",
        "        'logits':[tf.keras.metrics.MeanSquaredError(name='mse'), tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')],\n",
        "        'classes':[tf.keras.metrics.MeanIoU(num_classes=2, name = 'mean_iou')]\n",
        "        }\n",
        "\n",
        "OUT_DIR  = '{YOUR DIRECTORY FOR SAVING MODEL FILES HERE}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf1VcE0h9ZBj"
      },
      "source": [
        "When our training data is unbalanced it can be helpful to provide weights for the positive examples so that the model doesn't 'learn' to just predict zeros everywhere. To calculate the weight we read through the dataset and count up the number of 1s and 0s in our labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk2tu-Q6l613"
      },
      "source": [
        "# Instantiate a nonsense model\n",
        "m = get_model(depth = len(BANDS), optim = OPTIMIZER, loss = 'mse', mets = [tf.keras.metrics.categorical_accuracy], bias = None)\n",
        "train_con_mat = make_confusion_matrix(training, m)\n",
        "classums = train_con_mat.sum(axis = 1)\n",
        "\n",
        "# Calculate and save Bias, Weight, and Train size based on data\n",
        "BIAS = np.log(classums[1]/classums[0])\n",
        "WEIGHT = classums[0]/classums[1]\n",
        "TRAIN_SIZE = train_con_mat.sum()//(256*256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7ouh97-9qP7"
      },
      "source": [
        "During model training we will save the best performing set of weights as calculated on evaluation data at the end of each epoch. THe metric we track is the mean intersection over union."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlRa0mR6kRwY"
      },
      "source": [
        "## DEFINE CALLBACKS\n",
        "\n",
        "def get_weighted_bce(y_true, y_pred):\n",
        "    return weighted_bce(y_true, y_pred, WEIGHT)\n",
        "\n",
        "# get the current time\n",
        "now = datetime.now() \n",
        "date = now.strftime(\"%d%b%y\")\n",
        "date\n",
        "\n",
        "# define a checkpoint callback to save best models during training\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    os.path.join(OUT_DIR, 'best_weights_' + date + '.hdf5'),\n",
        "    monitor='val_classes_mean_iou',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode='max'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0FluNo_9xzL"
      },
      "source": [
        "Create and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmC4a8c7jfb3"
      },
      "source": [
        "m = get_model(depth = len(BANDS), optim = OPTIMIZER, loss = get_weighted_bce, mets = METRICS, bias = BIAS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mKm-2j3ki6u"
      },
      "source": [
        "# train the model\n",
        "m.fit(\n",
        "        x = training,\n",
        "        epochs = EPOCHS,\n",
        "        steps_per_epoch = int(TRAIN_SIZE//BATCH),\n",
        "        validation_data = evaluation,\n",
        "        callbacks = callbacks#,\n",
        "        #initial_epoch = initial_epoch\n",
        "        )\n",
        "\n",
        "m.save(os.path.join(OUT_DIR, f'{date}_unet256.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}